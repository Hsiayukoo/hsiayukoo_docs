# MindSpore OCR 部署工具需求设计说明书

## 一、修订记录

| ***\*日期\**** | ***\*修订版本\**** | ***\*修改章节\**** | ***\*修改描述\****                          | ***\*作者\****        |
| -------------- | ------------------ | ------------------ | ------------------------------------------- | --------------------- |
| 2024-08-19     | 1.0                |                    | 初稿完成                                    | hsiayukoo@outlook.com |
| 2024-08-20     | 2.0                |                    | 新增设计目的、概要、时间节奏及 gRPC实现方案 |                       |

[TOC]

缩略语清单

| ***\*缩略语\**** | ***\*英文全名\****            | ***\*中文解释\**** |
| ---------------- | ----------------------------- | ------------------ |
| OCR              | Optical Character Recognition | 光学字符识别       |
|                  |                               |                    |

 

## 二、简介

### 2.1 要做什么？

我们要做的是设计实现一个部署工具，我们希望它足够通用，能扩展到 MindSpore 的其他套件上。

### 2.2 为什么要做部署工具？

一个套件成功的标准即在于是否能帮助客户达成商业成功，是否帮助客户解决了开发生产中的问题。那么作为企业开发者，最关键的便是套件是否具备让开发者迅速训练搭建所需的模型，并且迅速部署起来，投入生产。

那么部署工具的开发，其目的便是**帮助用户快速部署相关模型，能够将训练好的深度学习模型转换和优化，能够在不同的环境和平台上高效、稳定地运行。**

### 2.3 部署工具该怎么做？

#### 2.3.1 借鉴现有的部署工具

- FastDeploy

FastDeploy 是百度开发的 AI 部署工具，提供了 **（1）模型量化压缩、转换、可视化工具（2）NLP、图像预处理工具（3）Serving 服务化部署框架（4）统一的多 IR ，多后端推理推理模块**。

![image](https://github.com/user-attachments/assets/62a22dbe-5525-4102-8235-95e88b3f5609)


- OpenVINO Toolkit

OpenVINO Toolkit 是 Intel 开发的模型部署工具，其框架大概如下，**（1）模型转换工具、各种插件（2）服务化部署框架（3）运行时**

![image](https://github.com/user-attachments/assets/747e87f1-1f46-4b63-b73f-52fb44a0fd90)


我们可以看出来，从模型架构来看，业界主流的模型部署工具主要包含了以下三个模块。

- **工具**：小型化、模型转换（如各个框架之间的模型之间相互转换）以及各种公共的图像、文本处理等工具。
- **服务化部署**：基于已有的服务化部署工具提供服务化部署能力。
- **运行时**：提供各种后端运行时。

#### 2.3.2 我们的方案

目前 MindSpore OCR 对各种部署方案的支持情况如下。

| 推理后端                  | 部署方案                     | 系统环境            | 硬件                                                         | 备注               | 模型格式           | 适用场景                                                     | 是否支持 |
| ------------------------- | ---------------------------- | ------------------- | ------------------------------------------------------------ | ------------------ | ------------------ | ------------------------------------------------------------ | -------- |
| MindSpore                 | 基于Python 的 OCR 在线推理   | Linux\MacOS\Windows | CPU\GPU\Asecnd                                               | 命令行方式在线推理 | `.ckpt`            | 演示、评估                                                   | 是       |
| MindSpore Lite            | 基于Python/C++ 的 OCR 推理   | Linux               | Ascend 310\ Ascend 310P                                      | 离线推理           | `.ms`              | 高性能 <font color =red>端侧推理</font>，直接推理也比上一种方式更快 | 是       |
| ACL                       | 同 MindSpore Lite            |                     |                                                              |                    | `.om`              |                                                              | 是       |
| MindSpore \MindSpore Lite | mindspore serving 服务化部署 | Linux               | MS 作为后端支持 Ascend 910 和 GPU \  MS Lite 作为后端支持  Ascend 310/310P 、CPU、GPU | 服务化部署         | `.mindir` 或 `.ms` | <font color = red>高并发、高性能、快速响应</font>            | 否       |

>  note:  (1) MindIR Lite 是 MindIR 针对端侧（如手机和嵌入式设备）优化的版本，对应的 mindIR 使用 `.mindir` 后缀的模型，mindir_lite 对应的后缀为 `.ms`

但是对于企业使用 MindOCR 的时候，<font color = red>更多面向的是服务化部署场景</font>。其主要原因是：**服务化部署可以提高并发处理能力、易于维护和升级、保障数据的安全性，实现资源的高效利用和快速响应客户需求。**

除此之外，借鉴 FastDeploy 、OpenVINO 工具，<font color = red>我们设计的远期的模型部署工具架构如下，但其中服务化部署时当前版本需要重点实现的。</font>

![image](https://github.com/user-attachments/assets/20a36452-9ce9-420e-9d83-b044f99f6500)


### 2.3 业界是怎么做服务化部署的？

我们参考 FastDeploy、Trtion inference server、TorchServe 是当前业界主流的服务化部署工具。

#### 2.3.1 FastDeploy 和 Triton inference server 怎么做服务化部署

FastDeploy 其实能够商业化使用的只有其中的基于 Triton inference sever 版本，只是其中推理 backend 使用的是 paddlepaddle ，所以我们只需要看 Triton inference server 是怎么做的。 我们以 python 推理后端为例（PyTorch 后端不允许直接调用自定义的 Python 代码，包括前后处理等）。

1. **模型导出 TorchScript 格式**：这里面包含模型的结构定义、forward、前处理、后处理函数。
2. **创建 Triton 模型仓库**：创建一个目录，将导出的模型放入其中，并创建一个 `config.pbtxt` 的配置文件。
3. **在 config.pbtxt 文件中指定输入输出格式、版本信息、后端类型等**：每一个模型都要编写对应的 `.pbtxt` 文件。
4. **编写 Python 后端文件**：主要是定义一个模型，实现 `initialize()` 和 `excute()` 方法，让 Triton 在执行的时候，会执行其中涉及的前后处理。
5. **启动 Triton server**：一行命令启动 triton server。
6. **部署到生产环境（可选）**：使用 Docker 或者 Kubenetes 进行容器化或编排。

#### 2.3.2 TorchServe 是怎么做的？

其主要步骤概括起来如下：

1. **模型导出为 TorchScript 格式**：torchScript 可以将模型前处理、后处理都写在模型定义里，（不用写到 forward，而是作为一个实例方法）
2. **创建模型的 .mar 文件**：torchserve 用 `.mar` 格式来打包模型。包含了模型的权重、结构、模型前后处理等逻辑
3. **启动 torch serve**：一行命令拉起
4. **部署到生产环境（可选）**

然后我们具体的来看看 torchserve 的结构和怎么部署模型的。torch serve 的 frontend 主要是 Java ，backend 主要是 Python。

#### 2.3.3 服务化部署流程总结

总结来看，无论是 Torch serve、fastDeploy（本质还是 Triton） 还是 Triton inference server，<font  color = red>其服务化部署思路都是一样的。</font>

![image](https://github.com/user-attachments/assets/15cc2c6d-73f0-47ea-a8f0-33b36f6ad2c4)



1. **导出模型文件：包含模型结构、前后处理、推理方法等**（前后处理可能在其他步骤实现）
2. **将其打包**：无论是 `.mar` 还是 Triton 的模型仓库，都是对模型进行打包。
3. **编写对应的的控制配置文件**：用来定义模型的输入输出、版本等信息。
4. **一行命令拉起服务**：拉起服务化部署。
5. **部署到生产环境（可选）**

#### 2.3.4 有什么可以借鉴的

1. **Torch serve 内置了 kubernetes 支持**

Kubernetes 和 Torch serve 等工具服务于不同层面，像一般的服务化部署工具只考虑单个模型或一组模型的部署和管理，适合小规模和单一场景的部署需求，而 Kubernetes 是一个容器编排平台，用于自动化部署、扩展、和管理容器化程序程序。适用于高可扩展、高可用、大规模多服务的场景。

2. **Triton serve 、Triton 等支持模型热更新**

模型热更新意味着不中断服务的情况下进行更新，包括热加载、模型版本切换等。适用于需要频繁更新模型的场景。

### 2.4 我们基于什么框架来做服务化部署？

#### 2.4.1 候选项

因为 MindSpore 的生态其实已经有一些成熟的服务化部署框架，**我们不需要从头去开发服务化部署框架**，像 PaddleOCR 一样，<font color = red>只是结合已有的服务化部署框架，为套件提供服务化部署解决方案，并可以针对性的做一些改进</font>，所以我们主要在 **MindSpore Serving、MindIE、Triton、FastAPI** 中进行服务化部署框架选择，进行对比。

#### 2.4.2 FastAPI 不适合商业化应用

在上述四个候选项中，FastAPI 并不是专门为机器学习\深度学习 提供的服务化部署框架，主要原因有以下几点。

- 性能不佳：FastAPI 是使用纯 Python 编写的服务化部署框架，性能并不太好。
- 不是针对深度学习设计的框架：FastAPI 是一个通用的服务化部署框架，无法与为深度学习专门设计的框架相比。
- 需要大量额外配置：FastAPI 是一个基础框架，许多新的功能都需要额外配置，对客户来说并不友好。

#### 2.4.3 Triton 属于远期规划

Triton 是英伟达旗下的商业化服务部署框架，如果其他深度学习框架需要使用，需要自行根据 Triton 提供的模板提供对应的推理后端，**工作量很大**，并且Trtion 主要还是适用于 Nvidia 的 GPU，对 Ascend 并不支持，故暂时不考虑使用 Triton 服务化部署框架。

#### 2.4.4 MindSpore Serving 比 MindIE 更适合 OCR 服务化部署

和 Mindformers 团队进行了交流，他们既使用了 MindSpore serving 服务化部署，也使用过 MindIE 服务化部署，对于 MindOCR 小模型来说，MindIE 并不是最合适的选择，相比之下，MindSpore Serving 更合适。结合自身体验及内部体验反馈，主要原因是以下几点。

- **MindIE 是专门为大模型服务化部署设计的**，很多优化都是针对 LLM 的。
- **MindIE 服务化部署使用非常复杂**，在周边反馈的体验不好，相关文档也并不齐全，对客户来说学习成本过高。
- **MindIE 接口仍在不断调整**，MindIE 尚未开源、仍在开发过程中，许多接口可能还会调整变更，如果使用 MindIE，维护成本将非常高。

#### 2.4.5  服务化部署框架选型结论

<font color = red>最后我们认为，MindSpore serving 是目前最适合 MindOCR 进行商业化服务化部署的</font>。并且 mindspore serving 的服务化部署步骤也符合前 2.3.5 中总结的步骤。举个例子，想要部署一个 resnet-50 模型。（[resnet-50部署](https://github.com/mindspore-ai/serving/tree/master/example/resnet)）

1. **导出模型的 mindir 文件**
2. **创建对应的模型文件夹**
3. **编写模型对应的 servable_config 文件**：这里会对模型的前处理、后处理进行定义，并且对模型的输入输出进行定义，并注册。
4. **启动服务**
5. **部署到生产环境（可选）**

### 2.5 运行状态

![image](https://github.com/user-attachments/assets/ad18b002-ff7c-4018-b542-f4f8a8d26abf)



- 我们通过 mindocr 来 **提供各种模型服务化部署的 docker file**。
- mindocr 提供给外部 **k8s 的配置**，方便客户进行服务化部署。（类似 torch serve）

那么关键就是做到哪一步，我们对标 TorchServe。torchserve 首先提供 Helm Charts 配置模板，其次提供北向平台配置模板，**那么我们当然优先支持 CCE ，也就是华为云的。**

> 注：TorchServe 的 kubernetes 目录结构如下

```shell
kubunertes
├── AKS/   # 提供部署到 Azure Kubernetes service 上的指南以及模板文件,
├── EKS/  # 提供部署到 Elastic Kubernetes service 上的指南和模板
├── GKE/  # Google Kubernetes Engine 上的指南和模板
├── Helm/  # 提供 Helm Charts 配置 yaml 模板
├── examples/  # 提供案例
├── images/  # 一些配图
├── kserve/  # kserve 是一个 k8s 为机器学习设计的，但是这里不支持 MS，所以我们没法用 kserve
├── tests/  # 测试案例
├── README.md
├── adapter.yaml  # 用于部署 K8S 集群中的 prometheus 适配器的 Helm 图标的默认值
├── autoscale.md
├── destination_rule.yaml  # 用于配置 Istio 服务网络中的流量路由和负载均衡策略。
├── gatgeway.yaml  # 定义了 Istio gateway 资源
├── hpa.yaml  # 配置了 HorizontalPodAutoscaler（HPA）资源
├── virtual_service.yaml  # 配置了 Istio 的 VirtualService 资源，用于控制和路由进入 K8S 服务的 HTTP 和 gRPC 流量
```

### 2.6 MindOCR 下新增目录结构

我们认为 ocr_serving 是工具，不是包，不应该被直接导入。整体逻辑结构如下（这里只绘制**单台服务器上单任务实例部署方式**，生产环境部署结合 k8s_support 里提供的 k8s 配置文件 和 docker 里面提供的 dockerfile 进行部署）：

![image](https://github.com/user-attachments/assets/44257c3a-9b58-4f38-a8f3-642d88cc7d92)



我们希望在 mindocr 里面嵌入 ocr_serving 到 deploy 目录下。

![image](https://github.com/user-attachments/assets/1afa70bb-f316-4352-82f5-04a3835f588a)



我们按优先级拆分各个模块：

#### 2.6.1 configs 模块

> 1. task_configs 模块

task_configs 主要是提供各种任务的默认配置 yaml 文件，其结构参考 mindocr.configs 下的 yaml 文件结构，用 Python 表示大概如下：

```python
{
    "det": List[DetConfig]
    "rec": List[RecConfig]
    ...
}
```

而 DetConfig 等类的数据结构如下所示，

```python
class DetConfig:
    model_name: str
    postprocess: mindocr.postprocess.det_base_postprocess.DetBasePostprocess
    preprocess: Preprocessor
    use_pretrained_mindir: bool
```

> 2. servables 模块

本模块主要是为各种任务提供 servable_config.py 文件的模板，任务分别是

- **ser**：semantic entities 任务。
- **rec**：text recognition 任务。
- **det**：detection 任务
- **sys**：detection + recognition 任务

我们以其中 **det** 任务为例，首先是 servable_config.py 文件结构：

```python
import numpy as np
from mindspore_serving.server import register

def read_yaml_config(path: str):
    # 从 yaml 中读取配置
    pass

# 获取 yaml 配置，包含模型名称、以及实例化 CoreModel 所需要的参数，还有下载对应的 mindir 文件对应的路径（可以自己决定下不下）
det_task_configs = read_yaml_config()

# 实例化 CoreModel
core_model = CoreModel(rec_task_configs) 


def preprocess(image: np.array):
    # 定义前处理方法
    return core_model.preprocess(image)
    

def postprocess(pred: [Tensor, Tuple[Tenosr], np.ndarray], shape_list: Union[np.ndarray, ms.Tensor] = None) -> dict:
    # 定义后处理方法
    return core_model.postprocess(pred, shape_list)
    
# 通过 det_task_configs 来获取mindir 的路径
model = register.declare_model(model_file=rec_task_configs.model_path, model_format="MindIR", with_batch_dim=False)


# register det_infer
@register.register_method(output_names=["det_res_final"])
def det_infer(image):  # only support float32 inputs
    x = register.add_stage(preprocess, ...)
    x = register.add_stage(model, ...)
    x = register.add_stage(postprocess, ...)
    return x
```

#### 2.6.2 package_utils 模块

本部分主要是将服务化部署要的文件进行打包，直接提供 mindspore serving 进行服务化部署所需要的文件夹。

```python
# file_name: package_helper.py 

import argparse
from typing import Any


OFFICIAL_TASK_FILE_PATH = "xxx"

def path_check(path: str) -> bool:
    # todo： 文件路径校验（mindir 文件地址、任务配置文件地址）
    return


def task_config_check(path: str) -> bool:
    # todo： 任务配置文件内容校验
    return

def get_task_config_from_yaml(task_name: str, model_name: str, task_file_path: str = OFFICIAL_TASK_FILE_PATH) -> dict[str, Any]:
    # todo: 通过 task_name，model_name 来提取信息
    return

def download_pretrained_mindir_file(task_name: str, model_name: str, donwload_path: str, save_path: str):
    return


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("task_name", help="需要部署的任务名称", required=True)
    parser.add_argument("model_name", help="模型名称", required=True)
    parser.add_argument("mindir_file_path", help="模型 mindir 文件地址，如果使用 mindocr 提供的文件，则不需要填写",
                        required=False)
    parser.add_argument("task_config_path", help="用户自定义的任务配置文件路径，如果使用 mindocr 提供的，则不用填写",
                        required=False)
    parser.add_argument("save_path", help="文件保存目录", required=False)
    args = parser.parse_args()

    task_name = args.task_name
    model_name = args.model
    mindir_file_path = ""

    # 1. 获取配置
    if args.task_file_path:
        if not path_check(args.task_file_path):
            raise Exception
        else:
            task_configs = get_task_config_from_yaml(task_name, model_name, task_file_path=args.task_file_path)
    else:
        task_configs = get_task_config_from_yaml(task_name, model_name)
        
    # 2. 获取 mindir 文件
    if args.mindir_file_path:
        if not path_check(args.mindir_file_path):
            raise Exception
        else:
            pass
    else:
        download_pretrained_mindir_file(task_name, model_name, task_configs.download_path, save_path=args.save_path)
    
    # 3. 打包对应的文件
    # todo ：打包对应的文件


```

